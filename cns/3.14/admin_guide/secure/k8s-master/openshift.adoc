== OpenShift API servers

//'''
//
//title: OpenShift API servers
//type: single
//url: "/3.14/secure/k8s-master/openshift/"
//weight: 20
//menu:
//  3.14:
//    parent: "k8s-master"
//    identifier: "openshift-api"
//canonical: https://docs.aporeto.com/saas/secure/k8s-master/openshift/
//aliases: [
//  "../setup/k8s-master/openshift/"
//]
//
//'''

=== Requirements

You need to have the following setup prepared and running before continuing with this guide:

* Installed and configured xref:../../start/apoctl/apoctl.adoc[apoctl]
* A running OpenShift cluster
* Installed and confirmed Aporeto Kubernetes setup as described xref:../../start/enforcer/k8s.adoc[here].

[NOTE]
====
For maximum security we recommend that you install the enforcers in your OpenShift cluster as an RPM and run `enforcerd` as a systemd service. This segregates the component enforcing security from your cluster and it avoids for it to become an attack vector from inside the cluster. You can find the instructions for how to run the enforcer on a Kubernetes node/master as a Linux service xref:../../../start/enforcer/linux[here]
====

=== Prepare apoctl

Make sure that for all the following commands in this guide, you point `apoctl` to the right Aporeto namespace.
Export the following variable to do so:

[,console]
----
# Replace /path/to/openshift/cluster with the namespace where your OpenShift cluster resides
export APOCTL_NAMESPACE=/path/to/openshift/cluster
----

Confirm that `apoctl` is now pointing to the right namespace as well as platform by running:

[,console]
----
apoctl api info
----

In the output the namespace should match the namespace that you set in the instructions above.

=== Create Aporeto resources

Before we can activate and create the Aporeto service, we have to create a Kubernetes API definition, as well as a host service and mapping.

Begin by creating the HTTP resource spec definition for the Kubernetes API:

[,console]
----
cat <<EOF | apoctl api create httpresourcespec -f -
name: kubernetes-api
description: "The Kubernetes API spec"
associatedTags:
- api=kubernetes
endpoints:
- URI: /*
  allowedScopes: []
  methods:
  - GET
  - POST
  - PUT
  - DELETE
  - PATCH
  - HEAD
  public: true
  scopes: []
EOF
----

[TIP]
====
This is the area where you can later further secure specific access to the Kubernetes API. By default and for a first installation, leave everything open and public. You can then afterwards look at which users/services access the API, and if you want to further lock them down.
====

Next, create a host service which essentially identifies the OpenShift master API server on master nodes.

[,console]
----
cat <<EOF | apoctl api create hostservice -f -
name: okd_api
description: "The OpenShift Master API service"
associatedTags:
- hostservice=openshift-api
services:
- tcp/8443
EOF
----

Afterwards, we need to map this host service to the enforcers which are running it. Use the following command for that:

[,console]
----
cat <<EOF | apoctl api create hostservicemappingpolicy -f -
name: okd_api
description: "The OpenShift Master API service"
object:
- - hostservice=openshift-api
subject:
- - $name=enforcer-name-1
- - $name=enforcer-name-2
EOF
----

[WARNING]
====
You need to replace `$name=enforcer-name-1` and `$name=enforcer-name-2` with the name of the enforcers that are running the master API server in your OpenShift cluster. Any other selector that matches all master nodes can be used as well, of course.
====

=== Prepare the master nodes

Add the Aporeto platform certificate authority (CA) to the allowed certificate authorities (CAs) in the front proxy configuration on all your master nodes:

[,console,subs="+attributes"]
----
curl -o - https://{ctrl-plane-api-url}/_meta/ca >> /etc/origin/master/front-proxy-ca.crt
----

Unfortunately, the configuration options for the request header options for the front proxy in the OpenShift master configuration are very limited.
This is why currently we need to list all processing units that implement our host service and put them into the `clientCommonNames` list.
Essentially the enforcer turns into the front proxy for the OpenShift master API server, and because the Master API server needs to listen on TLS itself, we need to present a client certificate from the enforcer when connecting upstream.
Based on the presented client certificate the Master API server will accept to take the username and group names from the request headers.
In Aporeto every processing unit that implements an Aporeto service has its own client certificate that is used for communication between different services.
This certificate will be presented when connecting to the Master API server upstream.
These certificates have the processing unit ID in their common name.

Therefore we are going to generate a list of all processing unit IDs now that implement our host service:

[,console]
----
apoctl api list pu -r -o table -c ID -f 'metadata contains @usr:hostservice=openshift-api'
----

The number of listed IDs should match the number of master servers.

Now we need to update the `authConfig` section in the master configuration at `/etc/origin/master/master-config.yml` and add all common names (CNs) of allowed certificates - which means all IDs of processing units that implement the host service. Find the relevant section in the file, and adjust it with the following:

[,yaml]
----
...
authConfig:
  requestHeader:
    clientCA: front-proxy-ca.crt
    clientCommonNames:
    - aggregator-front-proxy
    - 5c79de6a85092a00011e6590
    extraHeaderPrefixes:
    - X-Remote-Extra-
    groupHeaders:
    - X-Remote-Group
    usernameHeaders:
    - X-Remote-User
...
----

[WARNING]
====
Replace `5c79de6a85092a00011e6590` with the ID of your master, and make an entry for every ID that matched our `apoctl` query above.
====

Now you need to restart your OpenShift Master API server, at least. There are several ways to restart these components in a managed fashion. Refer to the https://docs.openshift.com/container-platform/[OpenShift Container Platform documentation] for the best method for your setup.

If you have a single master server, this can be achieved with a simple reboot:

[,console]
----
systemctl reboot
----

Your OpenShift Container Platform cluster is now prepared to be protected with Aporeto. The final step is now in the next section to create the Aporeto service and activate the protection.

=== Create the Aporeto service

Before you create the service, create a file in which you can more easily manage the configuration of the service.

Create a file _kubernetes-api-example.yaml_ with the following contents:

[,yaml]
----
name: "kubernetes-api"
description: "The Kubernetes API server"

type: HTTP

# This has to be set to JWT.
# It means that MTLS is tried regardless, but JWT will also be verified if present, and enforced if no MTLS has been done.
authorizationType: JWT

# put down all IPs under which the Kubernetes API server is accessed by which by default is:
# - the master server's IP address (external and internal as needed)
# - the default/kubernetes service IP (for in-cluster access)
IPs:
  # internal master node IP, replace with the correct one
  - 192.0.2.1
  # external master node IP, replace with the correct one, or remove completely if public access is not desired
  - 203.0.113.1
  # the kubernetes service cluster IP in the default namespace, replace with the correct one
  - 172.30.0.1

# put all the hosts here that the Kubernetes API is accessed by
# especially in-cluster, it is access through its DNS names
hosts:
  # the hostname as well as fqdn of the host, adjust as necessary
  - openshift-master-1
  - openshift-master-1.c.example.internal
  # all internal entries to reach the kubernetes service
  # you most likely want to keep these
  - kubernetes
  - kubernetes.default
  - kubernetes.default.svc
  - kubernetes.default.svc.cluster.local
  # all internal entries to reach the openshift service
  # you most likely want to keep these
  - openshift
  - openshift.default
  - openshift.default.svc
  - openshift.default.svc.cluster.local

# the Kubernetes API port where it listens
# kubeadm: 6443
# OpenShift: 8443
port: 8443
exposedPort: 8443

# random port on where you'll have to access the Kubernetes API from now on
# NOTE: this requires reconfiguration of your ~/.kube/config files
# NOTE: this requires reconfiguration of the default/kubernetes Service and Endpoint
#       replace the target port from its original 6443/8443 to this port here
publicApplicationPort: 443

# must be set to true: the Kubernetes API server always runs TLS
exposedServiceIsTLS: true

# because we need to run the same cert and keys as the Kubernetes API server
TLSType: External

# must be copied from the Kubernetes API server cert from:
# kubeadm: /etc/kubernetes/pki/apiserver.crt
# OpenShift: /etc/origin/master/master.server.crt
TLSCertificate: |
  -----BEGIN CERTIFICATE-----
  ...
  -----END CERTIFICATE-----
  -----BEGIN CERTIFICATE-----
  ...
  -----END CERTIFICATE-----

# must be copied from the Kubernetes API server key from:
# kubeadm: /etc/kubernetes/pki/apiserver.key
# OpenShift: /etc/origin/master/master.server.key
TLSCertificateKey: |
  -----BEGIN RSA PRIVATE KEY-----
  ...
  -----END RSA PRIVATE KEY-----

# put the public keys / certs here that are used for signing JWTs
# for Service Accounts
# kubeadm: /etc/kubernetes/pki/sa.key
# OpenShift: /etc/origin/master/serviceaccounts.public.key
#
# NOTE: Furthermore, also put all other public JWT signing keys here
# that are used to authenticate against Kubernetes. For example, if you
# are using Keycloak as an Identity Provider and you integrate it
# in Kubernetes through OIDC, then you want to export the public
# signing keys and put them here as well
JWTSigningCertificate: |
  -----BEGIN PUBLIC KEY-----
  ...
  -----END PUBLIC KEY-----
  -----BEGIN CERTIFICATE-----
  ...
  -----END CERTIFICATE-----

# put the Kubernetes CA cert here
# kubeadm: /etc/kubernetes/pki/ca.crt
# OpenShift: /etc/origin/master/ca.crt
MTLSCertificateAuthority: |
  -----BEGIN CERTIFICATE-----
  ...
  -----END CERTIFICATE-----

# put the Kubernetes CA cert here as well from
# kubeadm: /etc/kubernetes/pki/ca.crt
# OpenShift: /etc/origin/master/ca-bundle.crt
# This is needed because the enforcer establishes outgoing
# connections to the Kubernetes API server that need to be validated.
trustedCertificateAuthorities: |
  -----BEGIN CERTIFICATE-----
  ...
  -----END CERTIFICATE-----

# This is **vital** for the MTLS authentication to work
# NOTE: the targetHTTPHeader names must match the Kubernetes API server flags:
#       --requestheader-username-headers=X-Remote-User
#       --requestheader-group-headers=X-Remote-Group
claimsToHTTPHeaderMappings:
  - claimName: CN
    targetHTTPHeader: X-Remote-User
  - claimName: O
    targetHTTPHeader: X-Remote-Group

# match this to your HTTPResourceSpec definition of the Kubernetes API
exposedAPIs:
  - - api=kubernetes

# needs to select the Kubernetes API host service
selectors:
  - - hostservice=openshift-api
----

Now make a copy of this file:

[,console]
----
cp kubernetes-api-example.yaml kubernetes-api.yaml
----

And now edit the freshly copied _kubernetes-api.yaml_ and adjust and replace all values in there as required.
It can take quite some time to get all values together.
However, it is crucial that all values are correct before proceeding.

Once you are sure that you have adjusted all values correctly in the _kubernetes-api.yaml_ file, create the Aporeto service.

[,console]
----
apoctl api create service -f kubernetes-api.yaml
----

The OpenShift Master API is now protected with Aporeto and reachable at the _publicApplicationPort_.
Note that applications will still be able to reach the API on the original port without Aporeto protection.
If you want to force also all pod traffic to use the Aporeto protected port, you will have to adjust the Kubernetes service and endpoint objects.
Refer to <<_advanced,Advanced>> to learn more about this.

=== Update Kubernetes configuration

To start using the protected Master API endpoint, you are going to have to update the Kubernetes configuration and client configuration.
You can either do so by editing the configuration file directly, or you can do it with the following commands.

List the current available contexts in the configuration:

[,console]
----
oc config get-contexts
----

Identify the `cluster_ entry` name of the configuration from the list above and retrieve the currently configured server URL:

[,console]
----
oc config view -o jsonpath='{.clusters[?(@.name == "cluster-name")].cluster.server}{"\n"}'
----

[WARNING]
====
Make sure to replace _cluster-name_ in the above command with the cluster name from the list of the output from all available contexts.
====

Now update the server URL by replacing the port in the URL with the newly configured `publicApplicationPort` of the Aporeto service.

[,console]
----
oc config set-clusters cluster-name --server=https://openshift-master-1.c.example.internal:8443
----

[WARNING]
====
Make sure to replace `+https://openshift-master-1.c.example.internal:8443+` with the correct URL and `publicApplicationPort` of the Aporeto service.
====

Your Kubernetes configuration is now setup to use the protected OpenShift Master API endpoint.

[#_advanced]
=== Advanced

All topics in the advanced section of this guide are optional. However, they can add more visibility and security to your cluster.

==== Enable Master API protection for all pods

If you want to fully protect access to the Master API server with Aporeto, you can additionally configure the cluster internal service and endpoint to point to the Aporeto protected Master API service.

Get the Kubernetes service object:

[,console]
----
oc get service kubernetes -n default -o yaml
----

This should provide you with output similar to the following:

[,yaml]
----
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: 2019-03-01T09:05:25Z
  labels:
    component: apiserver
    provider: kubernetes
  name: kubernetes
  namespace: default
  resourceVersion: "622863"
  selfLink: /api/v1/namespaces/default/services/kubernetes
  uid: 26af8ff5-3c01-11e9-a039-42010a80000d
spec:
  clusterIP: 172.30.0.1
  ports:
  - name: https
    port: 443
    protocol: TCP
    targetPort: 8443
  - name: dns
    port: 53
    protocol: UDP
    targetPort: 8053
  - name: dns-tcp
    port: 53
    protocol: TCP
    targetPort: 8053
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
----

You need to update the `targetPort` of the `https` port in the definition above to match the `publicApplicationPort` of the Aporeto service. You can do this by running the following patch command:

[,console]
----
oc patch service kubernetes -p '{"spec":{"ports":[{"port":443,"targetPort":443}]}}' -n default
----

[WARNING]
====
Ensure that the `targetPort` really matches the `publicApplicationPort` of the Aporeto service.
====

Get the Kubernetes endpoints objects:

[,console]
----
oc get endpoints kubernetes -n default -o yaml
----

This should provide you with output similar to the following:

[,yaml]
----
apiVersion: v1
kind: Endpoints
metadata:
  creationTimestamp: 2019-03-01T09:05:25Z
  name: kubernetes
  namespace: default
  resourceVersion: "622891"
  selfLink: /api/v1/namespaces/default/endpoints/kubernetes
  uid: 26b872ec-3c01-11e9-a039-42010a80000d
subsets:
- addresses:
  - ip: 10.128.0.13
  ports:
  - name: dns
    port: 8053
    protocol: UDP
  - name: dns-tcp
    port: 8053
    protocol: TCP
  - name: https
    port: 8443
    protocol: TCP
----

You need to update the `port` of the `https` port in the definition above for all addresses to match the `publicApplicationPort` of the Aporeto service. You can do this by carefully running the following patch command:

[,console]
----
oc patch endpoints kubernetes --type json -p '[{"op": "replace", "path": "/subsets/0/ports/2/port", "value":443}]' -n default
----

[WARNING]
====
Ensure that the `path` of the JSON patch is correct. In this case it patch the first element of the subsets and the third element of the ports which coincidentally points to the `https` port in this example. Also ensure that the `value` will become the same value of the `publicApplicationPort` of the Aporeto service.
====

Good job!
Now all pod network communications with the OpenShift API go through the protected Aporeto service.
