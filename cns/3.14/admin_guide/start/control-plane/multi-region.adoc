== Multi-region deployments

:doctype: book
:experimental:

//'''
//
//title: Multi-region deployments
//type: single
//url: "/3.14/start/control-plane/multi-region/"
//weight: 40
//menu:
//  3.14:
//    parent: "control-plane"
//    identifier: "multi-region"
//on-prem-only: true
//
//'''

=== Overview

Deploying the Aporeto control plane across multiple regions ensures that your core services stay up after your region goes down.
If your region goes down, Aporeto fails over to a different region.
The clusters all have access to the same MongoDB, ensuring no interruption in the core services.

However, each cluster has its own time-series database.
You won't be able to access the flow logs, events, or reports of the original cluster from the failover cluster.
Likewise, after restoring operations in the original cluster, you'll have a gap in your flow logs, events, and reports.

In addition, any applications under *Integrations* won't be available from the failover cluster and automations may execute twice.

=== Requirements

* Three Kubernetes clusters in different regions
* Pod-to-pod communication across clusters on the following ports:
 ** 27017/27018/27019 tcp
 ** 53/udp
 ** 53/tcp
* Voila installed on a VM or workstation

[WARNING]
====
If your Kubernetes clusters are on EKS, you must deactivate SNAT.
====

=== Context

You can name your clusters as you wish, but we will refer to them using the following names.

* First cluster Kubernetes context identified as *test-active*
* Second cluster Kubernetes context identified as *test-passive*
* Third cluster Kubernetes context identified as *test-arbiter*.
This cluster will only host a few voters (arbiters) pods for the mongodb database.
It does not need to be large.

Note that you can only use menu:Integrations[Installed Apps] in a single zone.

=== Create your clusters

You can use a Terraform script for this, if needed.
Example for GKE:

[,console]
----
terraform {
  backend "gcs" {
    bucket = "cyril-test"
    prefix = "test-ha"
  }
}

provider "google" {
  version = "~> 2.20.2"
  credentials = "${file("~/.gcp/aporetotests.json")}"
  project     = "aporetotests"
}

module "aporeto-gke" {
  source = "git::https://github.com/aporeto-inc/tabularasa//modules/aporeto-gke"

  cluster_name = "${local.cluster_name}"
  location     = "${local.location}"

  disable_databases_node_pool  = "${local.disable_databases_node_pool}"
  disable_mongodb_node_pool    = "${local.disable_mongodb_node_pool}"
  disable_influxdb_node_pool   = "${local.disable_influxdb_node_pool}"
  disable_services_node_pool   = "${local.disable_services_node_pool}"
  disable_monitoring_node_pool = "${local.disable_monitoring_node_pool}"
  disable_highwind_node_pool   = "${local.disable_highwind_node_pool}"

  node_auto_scaling = false
  tags              = ["test-cyril"]

}

locals {
  env = "${terraform.workspace}"

  # Define our setups
  names = {
    "active"  = "test-active"
    "passive" = "test-passive"
    "arbiter" = "test-arbiter"
  }

  # zonal
  locations = {
    "active"  = "us-west1-a"
    "passive" = "us-central1-a"
    "arbiter" = "us-east1-b"
  }

  # nodes pools
  dont_create_node_pool_for_databases = {
    "active"  = "false"
    "passive" = "false"
    "arbiter" = "false"
  }

  dont_create_node_pool_for_mongodb = {
    "active"  = "false"
    "passive" = "false"
    "arbiter" = "true"
  }

  dont_create_node_pool_for_influxdb = {
    "active"  = "false"
    "passive" = "false"
    "arbiter" = "true"
  }

  dont_create_node_pool_for_services = {
    "active"  = "false"
    "passive" = "false"
    "arbiter" = "true"
  }

  dont_create_node_pool_for_monitoring = {
    "active"  = "false"
    "passive" = "false"
    "arbiter" = "true"
  }

  dont_create_node_pool_for_highwind = {
    "active"  = "false"
    "passive" = "true"
    "arbiter" = "true"
  }


  # set our vars
  cluster_name = "${lookup(local.names, local.env)}"
  location     = "${lookup(local.locations, local.env)}"

  disable_databases_node_pool  = "${lookup(local.dont_create_node_pool_for_databases, local.env)}"
  disable_mongodb_node_pool    = "${lookup(local.dont_create_node_pool_for_mongodb, local.env)}"
  disable_influxdb_node_pool   = "${lookup(local.dont_create_node_pool_for_influxdb, local.env)}"
  disable_services_node_pool   = "${lookup(local.dont_create_node_pool_for_services, local.env)}"
  disable_monitoring_node_pool = "${lookup(local.dont_create_node_pool_for_monitoring, local.env)}"
  disable_highwind_node_pool   = "${lookup(local.dont_create_node_pool_for_highwind, local.env)}"

}
----

This example will create three clusters in different regions as follows:

* `active` cluster has all node pools
* `passive` cluster has all node pools except highwind (as it cannot work in HA)
* `arbiter` cluster have only a database node pool

Use it as:

[,console]
----
terraform init
terraform workspace new active
terraform workspace new passive
terraform workspace new arbiter
for i in active passive arbiter; do terraform workspace select $i; terraform apply -auto-approve; done
----

=== Merge all your Kubernetes configuration files

If you have different files for the cluster, you can merge them together as follow:

[,console]
----
# Do backup of the existing ome
cp $HOME/.kube/config $HOME/.kube/config.backup.$(date +%Y-%m-%d.%H:%M:%S)

# Merge
KUBECONFIG=~/.kube/config:~/.kube/kubeconfig2 kubectl config view --merge --flatten > ~/.kube/kubeconfig-merged
mv ~/.kube/kubeconfig-merged ~/.kube/config
----

=== Create your voila environment

[NOTE]
====
If you have already deployed the control plane on the first cluster, that's fine.
====

Follow the documentation like any regular deployment but do not configure anything yet.

The rest of this procedure assumes that the namespace of the first cluster is `aporeto-svcs-active`.
If you already deployed the control plane on this cluster, it will be `aporeto-svcs` so please adapt accordingly.

=== Create your zones

Zones are Kubernetes clusters/namespaces that you can manage from a single voila environment.

To start we will create all the needed zones.
From the voila environment, create a new zone with the with the `mz` command.

[TIP]
====
`mz` allows you to manage multiple clusters from a single Voila environment.
====

Add a `passive` zone.

[,console]
----
mz -a passive test-passive aporeto-svcs-passive
----

Is should return something like the following.

----
* Initializing default zone... Ok
* Creating namespace aporeto-svcs-passive on test-passive Kuberentes cluster... Ok
* Adding new zone passive... Ok
----

[TIP]
====
Each Kubernetes cluster must have a unique name.
To avoid conflict with a pre-existing `aporeto-svcs` cluster, we use `aporeto-svcs-passive`.
====

Create another one for `arbiter`.

[,console]
----
mz -a arbiter test-arbiter aporeto-arbiter
----

Use the `mz list-zones` command to check the zones.

----
[DEFAULT] zone:

* Kubernetes context: test-active
* Kubernetes namespace: aporeto-svcs-active
* Voila configuration folder: /Users/cyril/Aporeto/voila-env/active/conf.d/DEFAULT
* Aporeto Zone: 0

[PASSIVE] zone (active):

* Kubernetes context: test-passive
* Kubernetes namespace: aporeto-svcs-passive
* Voila configuration folder: /Users/cyril/Aporeto/voila-env/active/conf.d/PASSIVE
* Aporeto Zone: 0

[ARBITER] zone:

* Kubernetes context: test-arbiter
* Kubernetes namespace: aporeto-arbiter
* Voila configuration folder: /Users/cyril/Aporeto/voila-env/active/conf.d/ARBITER
* Aporeto Zone: 0
----

[TIP]
====
When adding new zones the default Kubernetes context/namespace becomes the `default` zone.
====

The zones can be used in several ways.

* Using the `mz` wrapper, can be used to send a command to each zones (or `mz -z <zone>` to target a zone in particular).
* Using the `.  <(mz -e <zone>)` to enable a zone in the current shell (because not all commands can be wrapped around `mz` )

Make sure you can reach all your zones.

[,console]
----
mz k cluster-info
----

It should return something like the following.

----
[DEFAULT] k cluster-info

Kubernetes master is running at https://104.196.254.81

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

[PASSIVE] k cluster-info

Kubernetes master is running at https://35.232.243.139

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

[ARBITER] k cluster-info

Kubernetes master is running at https://34.66.34.154:6443

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
----

If not, then make sure your Kubernetes context and namespaces are correct.

[TIP]
====
The zone that was created first becomes the `default`.
In our case, this is the `active` cluster.
====

=== Verify that pod-to-pod communication is allowed

Use `mz` to list the pod CIDRs of your cluster.

----
mz k get nodes -o jsonpath='{.items[*].spec.podCIDR}'
----

Example output:

[,console]
----
[DEFAULT] k get nodes -o jsonpath={.items[*].spec.podCIDR}

10.80.0.0/24 10.80.1.0/24
[PASSIVE] k get nodes -o jsonpath={.items[*].spec.podCIDR}

10.88.1.0/24 10.88.0.0/24
[ARBITER] k get nodes -o jsonpath={.items[*].spec.podCIDR}

10.92.1.0/24 10.92.0.0/24%
----

The CIDRs used for pods on all clusters must be routable and allow the following ports.

* 27017/27018/27019 tcp
* 53/udp
* 53/tcp

=== Prepare your clusters

Enable Helm on your clusters.

[,console]
----
mz k create -n kube-system serviceaccount tiller
mz k create clusterrolebinding tiller-admin --clusterrole=cluster-admin --serviceaccount=kube-system:tiller
mz helm init --service-account tiller --upgrade --skip-refresh
----

Then you can proceed with the regular configuration for the `default` and `passive` cluster by prefixing with `mz` and the zone you want to configure.
Example:

----  
= Enter the d default zone

.  <(mz -e default)
set_value global.nodeAffinity.enabled true
set_value global.nodeAffinity.mode required
set_value global.prometheus.enabled true
set_value global.resources.enabled true
set_value global.rateLimiting.enabled true
set_value global.autoscaling.enabled true
----

[NOTE]
====
Skip that part if you already have a control plane running on the `default` zone.
====

Do the same for `passive` cluster but not for `arbiter`.
Example to create the storage class everywhere.

----
mz k apply -f storage.yaml
----

Configure the arbiter as follows.

[,console]
----
.  <(mz -e arbiter)
set_value router.replicas 0 mongodb-shard override
set_value config.storage.class standard mongodb-shard override
set_value config.storage.size 1 mongodb-shard override
set_value shard.storage.class standard mongodb-shard override
set_value shard.storage.size 1 mongodb-shard override
----

[NOTE]
====
For EKS it's `gp2`.
====

Do *not* run `upconf` yet.

=== Configure the MongoDB databases

[NOTE]
====
In the following commands if your `active` Kubernetes namespace is not `aporeto-svcs-active` then replace it with the correct one.
====

Configure the `passive` MongoDB instances.

[,console]
----
.  <(mz -e passive)
# tell that this config instances will rejoin the active ones (adapt namespace if required)
set_value config.join "mongodb-shard-config.aporeto-svcs-active" mongodb-shard override
----

Configure the `arbiter` MongoDB instances.

[,console]
----
.  <(mz -e arbiter)
# tell that we want 1 replicas for config instance
set_value config.replicas 1 mongodb-shard override
# tell that this config instances will rejoin the active ones (adapt namespace if required)
set_value config.join "mongodb-shard-config.aporeto-svcs-active" mongodb-shard override
----

Set an `N` environment variable containing the number of shards you have or plan to have.
To determine how many shards you have now, issue the following command.

[,console]
----
mz -z default get_value 'shard.shards[0].shards' mongodb-shard
----

If you do not plan to increase the number of shards, set `N` to the value returned.
Otherwise, set `N` to the desired number of shards.
In the following example, we set `N` to `1`.

[,console]
----
export N=1
----

Use the following commands to configure the shards in the `default` zone.

[NOTE]
====
If you already have a running control plane on the default zone, skip this.
====

[,console]
----
.  <(mz -e default)
# Set the shard name
set_value 'shard.shards[0].name' "mongodb-shard-data" mongodb-shard override
# Set the number of replicas to 3 on the active zone
set_value 'shard.shards[0].replicas' "3" mongodb-shard override
# Set the ReplicaSet name
set_value 'shard.shards[0].rs' "shard" mongodb-shard override
# Set the number of shards you want
set_value 'shard.shards[0].shards' "$N" mongodb-shard override
----

Use the following commands to configure the shards in the `passive` zone.

[,console]
----
.  <(mz -e passive)
# Set the shard name
set_value 'shard.shards[0].name' "mongodb-shard-data" mongodb-shard override
# Set the number of replicas to 3 on the passive zone
set_value 'shard.shards[0].replicas' "3" mongodb-shard override
# Set the ReplicaSet name
set_value 'shard.shards[0].rs' "shard" mongodb-shard override
# Set the number of shards
set_value 'shard.shards[0].shards' "$N" mongodb-shard override
# Instruction the data shard to join you `default` active zone (adapt namespace if needed)
set_value 'shard.shards[0].join' "mongodb-shard-data-0.aporeto-svcs-active" mongodb-shard override
----

Use the following commands to configure the shards in the `arbiter` zone.

[,console]
----
.  <(mz -e arbiter)
# Set the shard name
set_value 'shard.shards[0].name' "mongodb-shard-data" mongodb-shard override
# Set the replicas to be one
set_value 'shard.shards[0].replicas' "1" mongodb-shard override
# Set the ReplicaSet name
set_value 'shard.shards[0].rs' "shard" mongodb-shard override
# Set the number of shards
set_value 'shard.shards[0].shards' "$N" mongodb-shard override
# Instruct the mongodb instance to not initialize
set_value 'shard.shards[0].noinit' true mongodb-shard override
----

Check your `default` configuration.

[,console]
----
cat conf.d/DEFAULT/mongodb-shard/config.yaml
----

It should look something like the following.

----
shard:
  shards:

* shards: 1
name: mongodb-shard-data
replicas: 3
rs: shard
  storage:
class: fast
config:
  storage:
class: fast
----

Check your `passive` configuration.

[,console]
----
cat conf.d/PASSIVE/mongodb-shard/config.yaml
----

It should look something like the following.

----
config:
  replicas: 3
  join: mongodb-shard-config.aporeto-svcs-active
  storage:
    class: fast
shard:
  shards:

* shards: 1
name: mongodb-shard-data
replicas: 3
rs: shard
join: mongodb-shard-data-0.aporeto-svcs-active
  storage:
class: fast
----

Check your `arbiter` configuration.

[,console]
----
cat conf.d/ARBITER/mongodb-shard/config.yaml
----

It should look something like the following.

----
router:
  replicas: 0
config:
  storage:
    class: standard
    size: 1
  join: mongodb-shard-config.aporeto-svcs-active
  replicas: 1
shard:
  storage:
    class: standard
    size: 1
  shards:

* noinit: true
shards: 1
name: mongodb-shard-data
replicas: 1
rs: shard
----

=== Configure your endpoint URLs

The default endpoints are still managed via the `aporeto.yaml` file.
Use the following command to check them.

[,console]
----
mz -z default get_value global.public
----

Locate the `api` and `ui` keys.

----
...
api: https://35.247.3.115
ui: https://104.199.123.129
...
----

You can override the values using `mz`.
For example, the `passive` endpoints are set via the following command:

----
mz -z passive set_value global.public.api https://23.251.148.79 global override
mz -z passive set_value global.public.ui https://35.239.68.15 global override
----

At this point you can run `upconf`.
Or if you already have a setup deployed on the `default` cluster `upconf regen-certs`.

It may return something like the following.

----
2020-02-14 16:18:41 [Checking configuration...] done
2020-02-14 16:18:42 [Checking Certificate Authorities...] done
2020-02-14 16:18:43 [Checking External services...] done
2020-02-14 16:18:44 [Checking Private certificates...] done
2020-02-14 16:18:50 [Checking Public certificates...] done
  Entry not found as Subject Alertnative Name in certificate services.public-cert.pem: `35.247.3.115`
  I will not update the current public certificates.
2020-02-14 16:18:52 [success] configuration aligned
----

This indicates that you need to update the public certificate.
If you are using the one generated by Voila during the deployment, just remove it.

----
rm certs/services.public-*
----

Then rerun `upconf`.

[,console]
----
upconf
----

It should return a success message like the following.

[,console]
----
2020-02-14 16:20:30 [Checking configuration...] done
2020-02-14 16:20:31 [Checking Certificate Authorities...] done
2020-02-14 16:20:31 [Checking External services...] done
2020-02-14 16:20:33 [Checking Private certificates...] done
2020-02-14 16:20:38 [Checking Public certificates...] done
INFO[0000] certificate key pair created                  cert=services.public-cert.pem key=services.public-key.pem
2020-02-14 16:20:40 [success] configuration aligned
----

=== Connect the clusters

The cluster federation is done at the DNS level using `cerberus`.
The `cerberus` configuration is simple but requires two steps.
Each `cerberus` must know:

* other `cerberus` instances called *peers*
* the `services` to expose to other *peers*

=== Deploy cerberus

Enable and install cerberus.

[,console]
----
mz set_value enabled true cerberus override
mz snap -n cerberus
----

If you are running on EKS you should also run the following.

[,console]
----
mz set_value "annotations.[service.beta.kubernetes.io/aws-load-balancer-type]" nlb cerberus override
mz set_value "annotations.[service.beta.kubernetes.io/aws-load-balancer-internal]" "0.0.0.0/0" cerberus override
----

Once done gather the peers as follows.

[,console]
----
mz k get svc cerberus
----

It should return something like the following.

[,console]
----
[DEFAULT] k get svc cerberus

NAME       TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE
cerberus   LoadBalancer   10.84.7.232   34.82.245.0   443:30008/TCP   41m

[PASSIVE] k get svc cerberus

NAME       TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)         AGE
cerberus   LoadBalancer   10.0.20.157   23.251.148.79   443:30262/TCP   100s

[ARBITER] k get svc cerberus

NAME       TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)         AGE
cerberus   LoadBalancer   10.0.33.118   35.190.163.37   443:30160/TCP   64s
----

Wait until you have external IPs (or ELBs) for each one of the load balancers.

Use the following commands to enable the `default` zone to communicate with the `passive` and `arbiter` zones.

[,console]
----
.  <(mz -e default)
# Instruct cerberus on default (active) zone to reach peers from passive and arbiter zone
set_value peers "23.251.148.79 35.190.163.37" cerberus override
# Instruct cerberus to expose the following servies to other peers. (adapt namespace if needed)
set_value exposed_services "mongodb-shard-config.aporeto-svcs-active.svc.cluster.local mongodb-shard-data-0.aporeto-svcs-active.svc.cluster.local" cerberus override
----

Use the following commands to enable the `passive` zone to communicate with the `default` and `arbiter` zones.

[,console]
----
.  <(mz -e passive)
# Instrcut cerberus on passive zone to reach peers from default (active) and arbiter zone
set_value peers "34.82.245.0 35.190.163.37" cerberus override
# Instruct cerberus to expose the following servies to other peers.
set_value exposed_services "mongodb-shard-config.aporeto-svcs-passive.svc.cluster.local mongodb-shard-data-0.aporeto-svcs-passive.svc.cluster.local" cerberus override
----

Use the following commands to enable the `arbiter` zone to communicate with the `passive` and `default` zones.

[,console]
----
.  <(mz -e arbiter)
# Instrcut cerberus on arbiter zone to reach peers from default (active) and passive zone
set_value peers "23.251.148.79 34.82.245.0" cerberus override
# Instruct cerberus to expose the following servies to other peers.
set_value exposed_services "mongodb-shard-config.aporeto-arbiter.svc.cluster.local mongodb-shard-data-0.aporeto-arbiter.svc.cluster.local" cerberus override
----

If you have more than one shard add `mongodb-shard-data-N.aporeto-svcs-active.svc.cluster.local` with N from 0 to the number of shard -1 to each line.
Example if I have 3 shards this translate to the following for the `default` zone.

[,console]
----
set_value exposed_services "mongodb-shard-config.aporeto-svcs-active.svc.cluster.local mongodb-shard-data-0.aporeto-svcs-active.svc.cluster.local mongodb-shard-data-1.aporeto-svcs-active.svc.cluster.local mongodb-shard-data-2.aporeto-svcs-active.svc.cluster.local" cerberus override
----

Check your `default` configuration.

[,console]
----
cat conf.d/DEFAULT/cerberus/config.yaml
----

It should look like the following.

----
enabled: true
peers: 23.251.148.79 35.190.163.37
exposed_services: mongodb-shard-config.aporeto-svcs-active.svc.cluster.local mongodb-shard-data-0.aporeto-svcs-active.svc.cluster.local
----

Check your `passive` configuration.

----
cat conf.d/DEFAULT/cerberus/config.yaml
----

It should look like the following.

----
enabled: true
peers: 34.82.245.0 35.190.163.37
exposed_services: mongodb-shard-config.aporeto-svcs-passive.svc.cluster.local mongodb-shard-data-0.aporeto-svcs-passive.svc.cluster.local
----

Check your `arbiter` configuration.

----
cat conf.d/ARBITER/cerberus/config.yaml
----

It should look like the following.

----
enabled: true
peers: 23.251.148.79 34.82.245.0
exposed_services: mongodb-shard-config.aporeto-arbiter.svc.cluster.local mongodb-shard-data-0.aporeto-arbiter.svc.cluster.local
----

Run `upconf` again to adapt the cerberus certificates to the external IP or ELB they got.

----
2020-02-13 19:33:09 [Checking configuration...] done
2020-02-13 19:33:09 [Checking Certificate Authorities...] done
2020-02-13 19:33:10 [Checking External services...] done
2020-02-13 19:33:13 [Checking Private certificates...] done
  Entry not found as Subject Alertnative Name in certificate cerberus-cert.pem: `34.82.245.0`
  Entry not found as Subject Alertnative Name in certificate cerberus-cert.pem: `23.251.148.79`
  Entry not found as Subject Alertnative Name in certificate cerberus-cert.pem: `35.190.163.37`
INFO[0000] certificate key pair created                  cert=cerberus-cert.pem key=cerberus-key.pem
2020-02-13 19:33:20 [Checking Public certificates...] done
2020-02-13 19:33:22 [success] configuration aligned
----

Then update the `cerberus` with:

[,console]
----
 mz snap -u -n cerberus --force
----

At this point the DNS federation should work as expected.

=== Install MongoDB

To install MongoDB, use the following command.

[,console]
----
mz snap -n mongodb-shard
----

This will deploy MongoDB on the three clusters with their respective configurations.

Once done you can check the pod status with `mz k get pod`, this should look like this for one shard configuration.

[,console]
----
[DEFAULT] k get pod

NAME                       READY   STATUS    RESTARTS   AGE
cerberus-f479f6d4b-l72ld   1/1     Running   0          75m
mongodb-shard-config-0     1/1     Running   0          60m
mongodb-shard-config-1     1/1     Running   0          60m
mongodb-shard-config-2     1/1     Running   0          59m
mongodb-shard-data-0-0     1/1     Running   0          60m
mongodb-shard-data-0-1     1/1     Running   0          60m
mongodb-shard-data-0-2     1/1     Running   0          60m
mongodb-shard-router-0     1/1     Running   0          60m
mongodb-shard-router-1     1/1     Running   0          60m
mongodb-shard-router-2     1/1     Running   0          60m

[PASSIVE] k get pod

NAME                       READY   STATUS    RESTARTS   AGE
cerberus-dd69b698b-724xh   1/1     Running   0          66m
mongodb-shard-config-0     1/1     Running   0          59m
mongodb-shard-config-1     1/1     Running   0          59m
mongodb-shard-config-2     1/1     Running   0          59m
mongodb-shard-data-0-0     1/1     Running   0          59m
mongodb-shard-data-0-1     1/1     Running   0          59m
mongodb-shard-data-0-2     1/1     Running   0          59m
mongodb-shard-router-0     1/1     Running   0          59m
mongodb-shard-router-1     1/1     Running   0          59m
mongodb-shard-router-2     1/1     Running   0          59m

[ARBITER] k get pod

NAME                        READY   STATUS    RESTARTS   AGE
cerberus-574fc57dc4-7np6b   1/1     Running   0          65m
mongodb-shard-config-0      1/1     Running   0          59m
mongodb-shard-data-0-0      1/1     Running   0          59m
----

Then with `mz -z default mgos status` it should look like this:

[,console]
----
MongoDB status

* Sharding status:

Shard shard-z0-0 tagged as [z0] members
 - mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-active:27018
 - mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-passive:27018
 - mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-active:27018
 - mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-passive:27018
 - mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-active:27018
 - mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-passive:27018

* Config node ReplicaSet:

mongodb-shard-config-0.mongodb-shard-config.aporeto-svcs-active:27019 PRIMARY
mongodb-shard-config-1.mongodb-shard-config.aporeto-svcs-active:27019 SECONDARY
mongodb-shard-config-2.mongodb-shard-config.aporeto-svcs-active:27019 SECONDARY
mongodb-shard-config-0.mongodb-shard-config.aporeto-svcs-passive:27019 SECONDARY
mongodb-shard-config-1.mongodb-shard-config.aporeto-svcs-passive:27019 SECONDARY
mongodb-shard-config-2.mongodb-shard-config.aporeto-svcs-passive:27019 SECONDARY
mongodb-shard-config-0.mongodb-shard-config.aporeto-arbiter:27019 SECONDARY

* Data shard 0 mongodb-shard-data node ReplicaSet:

mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-active:27018 PRIMARY
mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-active:27018 SECONDARY
mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-active:27018 SECONDARY
mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-passive:27018 SECONDARY
mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-passive:27018 SECONDARY
mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-passive:27018 SECONDARY
----

Now we need to add the arbiter to the ReplicaSet data nodes:

[,console]
----
.  <(mz -e default)
# This command will instruct the data nodes to add an arbiter
mgos d eval 0 0 0 'rs.addArb("mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-arbiter:27018")'
----

This should print something like:

[,console]
----
<debug log stripped>
{
  "ok" : 1,
  "$gleStats" : {
    "lastOpTime" : {
      "ts" : Timestamp(1581639553, 1),
      "t" : NumberLong(41)
    },
    "electionId" : ObjectId("7fffffff0000000000000029")
  },
  "lastCommittedOpTime" : Timestamp(1581639548, 1),
  "$configServerState" : {
    "opTime" : {
      "ts" : Timestamp(1581639546, 3),
      "t" : NumberLong(41)
    }
  },
  "$clusterTime" : {
    "clusterTime" : Timestamp(1581639553, 1),
    "signature" : {
      "hash" : BinData(0,"ELE6SEqnBIQWvL0pYXjYwmni7gA="),
      "keyId" : NumberLong("6792969800621490185")
    }
  },
  "operationTime" : Timestamp(1581639553, 1)
}
----

Then with `mz -z default mgos status` it should now look like this.

[,console]
----
MongoDB status

* Sharding status:

Shard shard-z0-0 tagged as [z0] members
 - mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-active:27018
 - mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-passive:27018
 - mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-active:27018
 - mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-passive:27018
 - mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-active:27018
 - mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-passive:27018

* Config node ReplicaSet:

mongodb-shard-config-0.mongodb-shard-config.aporeto-svcs-active:27019 PRIMARY
mongodb-shard-config-1.mongodb-shard-config.aporeto-svcs-active:27019 SECONDARY
mongodb-shard-config-2.mongodb-shard-config.aporeto-svcs-active:27019 SECONDARY
mongodb-shard-config-0.mongodb-shard-config.aporeto-svcs-passive:27019 SECONDARY
mongodb-shard-config-1.mongodb-shard-config.aporeto-svcs-passive:27019 SECONDARY
mongodb-shard-config-2.mongodb-shard-config.aporeto-svcs-passive:27019 SECONDARY
mongodb-shard-config-0.mongodb-shard-config.aporeto-arbiter:27019 SECONDARY

* Data shard 0 mongodb-shard-data node ReplicaSet:

mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-active:27018 PRIMARY
mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-active:27018 SECONDARY
mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-active:27018 SECONDARY
mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-passive:27018 SECONDARY
mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-passive:27018 SECONDARY
mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-passive:27018 SECONDARY
mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-arbiter:27018 ARBITER
----

[NOTE]
====
Arbiters are valid only for data nodes, this is why we didn't add any arbiter for the nodes.
For the node that's a regular ReplicaSet.
====

=== Deploy your control plane

Just run:

* `mz -z default snap -n` to deploy the backend on the active cluster
* `mz -z passive snap -n` to deploy the backend on the passive cluster

You don't need to run anything else on the `arbiter` zone.

=== Sanity checks

Check that you can reach the web interface on the `default` zone endpoints and create an account (or login if the setup was existing).

Confirm that you can reach the web interface on the `passive` zone and try to login again.
It should work.

From the `default` zone web interface, create an object.
It should not be pushed to the `passive` web interface.
However, if you refresh the view you should see it.

=== Simulate a failover

To simulate an issue, and trigger failover, we will cut communication from and to the `default` cluster.
From the `passive` zone look at `mgos status` first to see where are the primaries.

[,console]
----
MongoDB status

* Sharding status:

Shard shard-z0-0 tagged as [z0] members
 - mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-active:27018
 - mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-passive:27018
 - mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-active:27018
 - mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-passive:27018
 - mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-active:27018
 - mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-passive:27018

* Config node ReplicaSet:

mongodb-shard-config-0.mongodb-shard-config.aporeto-svcs-active:27019 PRIMARY
mongodb-shard-config-1.mongodb-shard-config.aporeto-svcs-active:27019 SECONDARY
mongodb-shard-config-2.mongodb-shard-config.aporeto-svcs-active:27019 SECONDARY
mongodb-shard-config-0.mongodb-shard-config.aporeto-svcs-passive:27019 SECONDARY
mongodb-shard-config-1.mongodb-shard-config.aporeto-svcs-passive:27019 SECONDARY
mongodb-shard-config-2.mongodb-shard-config.aporeto-svcs-passive:27019 SECONDARY
mongodb-shard-config-0.mongodb-shard-config.aporeto-arbiter:27019 SECONDARY

* Data shard 0 mongodb-shard-data node ReplicaSet:

mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-active:27018 PRIMARY
mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-active:27018 SECONDARY
mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-active:27018 SECONDARY
mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-passive:27018 SECONDARY
mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-passive:27018 SECONDARY
mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-passive:27018 SECONDARY
mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-arbiter:27018 ARBITER
----

We can see that configuration and data primaries are on the `default` (active) zone.

Now change the firewall rules to disable traffic to the `default` (active) zone pod CIDR.

[,console]
----
# list all pod cidr
mz k get nodes -o jsonpath='{.items[*].spec.podCIDR}'

[DEFAULT] k get nodes -o jsonpath={.items[*].spec.podCIDR}

10.80.0.0/24 10.80.1.0/24
[PASSIVE] k get nodes -o jsonpath={.items[*].spec.podCIDR}

10.88.1.0/24 10.88.0.0/24
[ARBITER] k get nodes -o jsonpath={.items[*].spec.podCIDR}

10.92.1.0/24 10.92.0.0/24%
----

In this case I removed the rule to allow traffic from `10.80.0.0/24` and `10.80.1.0/24`.
After few seconds, if you go the the `default` active web interface, it will not load objects anymore and might throw an error such as the following.

[,console]
----
Unable to execute query: Could not find host matching read preference { mode: "primaryPreferred" } for set rscfg0
----

[,console]
----
Unable to execute query: failed on: shard-z0-0 :: caused by :: Could not find host matching read preference { mode: "nearest" } for set shard-z0-0
----

While the `passive` web interface can still load objects.
If you run `mgos status` from the `passive` zone  it should show the following.

[,console]
----
MongoDB status

* Sharding status:

Shard shard-z0-0 tagged as [z0] members
 - mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-active:27018
 - mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-passive:27018
 - mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-active:27018
 - mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-passive:27018
 - mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-active:27018
 - mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-passive:27018

* Config node ReplicaSet:

mongodb-shard-config-0.mongodb-shard-config.aporeto-svcs-active:27019 (not reachable/healthy)
mongodb-shard-config-1.mongodb-shard-config.aporeto-svcs-active:27019 (not reachable/healthy)
mongodb-shard-config-2.mongodb-shard-config.aporeto-svcs-active:27019 (not reachable/healthy)
mongodb-shard-config-0.mongodb-shard-config.aporeto-svcs-passive:27019 SECONDARY
mongodb-shard-config-1.mongodb-shard-config.aporeto-svcs-passive:27019 PRIMARY
mongodb-shard-config-0.mongodb-shard-config.aporeto-arbiter:27019 SECONDARY
mongodb-shard-config-2.mongodb-shard-config.aporeto-svcs-passive:27019 SECONDARY

* Data shard 0 mongodb-shard-data node ReplicaSet:

mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-active:27018 (not reachable/healthy)
mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-active:27018 (not reachable/healthy)
mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-active:27018 (not reachable/healthy)
mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-passive:27018 PRIMARY
mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-passive:27018 SECONDARY
mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-arbiter:27018 ARBITER
mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-passive:27018 SECONDARY
----

We can see that the `default` active MongoDB instances are not reachable and that primaries switched to the `passive` zone.

[NOTE]
====
The  `mgos status` from the `default` active zone will hang, that's normal.
====

Now you can put back the pod CIDR to the firewall allowed list and issue `mgos status` again from `default` (active ) or `passive` zone:

[,console]
----
MongoDB status

* Sharding status:

Shard shard-z0-0 tagged as [z0] members
 - mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-active:27018
 - mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-passive:27018
 - mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-active:27018
 - mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-passive:27018
 - mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-active:27018
 - mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-passive:27018

* Config node ReplicaSet:

mongodb-shard-config-0.mongodb-shard-config.aporeto-svcs-active:27019 PRIMARY
mongodb-shard-config-1.mongodb-shard-config.aporeto-svcs-active:27019 SECONDARY
mongodb-shard-config-2.mongodb-shard-config.aporeto-svcs-active:27019 SECONDARY
mongodb-shard-config-0.mongodb-shard-config.aporeto-svcs-passive:27019 SECONDARY
mongodb-shard-config-1.mongodb-shard-config.aporeto-svcs-passive:27019 SECONDARY
mongodb-shard-config-0.mongodb-shard-config.aporeto-arbiter:27019 SECONDARY
mongodb-shard-config-2.mongodb-shard-config.aporeto-svcs-passive:27019 SECONDARY

* Data shard 0 mongodb-shard-data node ReplicaSet:

mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-active:27018 PRIMARY
mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-active:27018 SECONDARY
mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-active:27018 SECONDARY
mongodb-shard-data-0-1.mongodb-shard-data-0.aporeto-svcs-passive:27018 SECONDARY
mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-svcs-passive:27018 SECONDARY
mongodb-shard-data-0-0.mongodb-shard-data-0.aporeto-arbiter:27018 ARBITER
mongodb-shard-data-0-2.mongodb-shard-data-0.aporeto-svcs-passive:27018 SECONDARY
----

Everything is back to normal.

=== Checking if active is down

To check if the platform is fully operational, issue a fake `issue` call such as the following.

[,console]
----
curl --max-time 10  -k -H "Content-Type: application/json" -X POST --data-binary '{"realm":"Vince","metadata":{"vinceAccount":"foo","vincePassword":"bar"},"validity":"720h"}'  https://35.247.3.115/issue
----

If the `default` active zone cannot communicate with `passive` or `arbiter`, the request times out.

----
curl: (28) Operation timed out after 10002 milliseconds with 0 bytes received
----

If all is well, it returns something like the following.


----
curl --max-time 10  -k -H "Content-Type: application/json" -X POST --data-binary '{"realm":"Vince","metadata":{"vinceAccount":"foo","vincePassword":"bar"},"validity":"720h"}'  https://35.247.3.115/issue
[{"code":401,"data":null,"description":"You are not authorized to access this resource","subject":"vince","title":"Unauthorized","trace":"unknown"}]%
----

=== Troubleshooting

If `mgos status` doesn't report correctly make sure that:

* You can reach your pod from another cluster
* The firewall doesn't prevent you from reaching those pods
* DNS federation works

To try that, once you have `cerberus`  and `mongodb-shard` deployed, you can simply run on for instance `passive` zone:

[,console]
----
k run -i --tty --rm debug --image=alpine --restart=Never -- sh
apk add bind-tools curl

# to resolve an active pod
dig mongodb-shard-config-0.mongodb-shard-config.aporeto-svcs-active.svc.cluster.local +short
10.80.0.11

# try to reach that pod
curl mongodb-shard-config-0.mongodb-shard-config.aporeto-svcs-active.svc.cluster.local:27019
It looks like you are trying to access MongoDB over HTTP on the native driver port.
----

If this does not return an IP, there is an issue somewhere. Look at the `cerberus` configuration and logs.
