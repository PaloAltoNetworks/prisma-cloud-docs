== Learning Identity-Based Microsegmentation

//'''
//
//title: Learn
//type: list
//url: "/3.14/learn/"
//menu:
//  3.14:
//    identifier: learn
//    weight: 50
//canonical: https://docs.aporeto.com/saas/learn/
//
//'''

=== Securing a micro-service application on Kubernetes

In this tutorial, you will learn how to secure a micro-service application with Aporeto.
We'll use a slightly modified version of https://github.com/GoogleCloudPlatform/microservices-demo[Google's Hipster Shop] as our micro-service application.
The following diagram illustrates its architecture.

image::hipster-shop-arch-diag.png[Architecture of hipster shop]

Once you complete <<_setting-up-your-cluster,Setting up your cluster>> and <<_deploying-the-hipster-shop-application,Deploying the hipster shop application>>, you can complete any one of sections I-IV.
However, we recommend completing these sections in sequence.
Doing so will show how you can develop your network policies in a development environment and move them to your production environment, matching the life-cycle progression of a microservices application.

As you move through the sections of this tutorial, the requirements increase.
Sections III and IV require `apoctl`.
Section IV requires a second Kubernetes cluster.

[#_setting-up-your-cluster]
=== Setting up your cluster

==== Prerequisites

===== Required

* https://microsegmentation.acme.com/register[Access to the Aporeto web interface]
* A Kubernetes version 1.9+ cluster
** *Minimum:* 2 `vCPUs` per worker node
** *Minimum:* 8 `vCPUs` total in the cluster
* `kubectl` installed and configured
* https://helm.sh/docs/using_helm/#installing-the-helm-client[Helm CLI installed]

===== Optional

* A second Kubernetes cluster
** *Minimum:* 2 `vCPUs` per worker node
** *Minimum:* 3 `vCPUs` total in the cluster
* `apoctl`

==== Installing Aporeto

You can use any of the following methods to install Aporeto on your Kubernetes cluster(s).

* https://microsegmentation.acme.com/app[Aporeto web interface quickstart]: The easiest method because it does not require `apoctl`.
To access this quickstart, click the *Rocket* icon in the top right and select *Secure a Kubernetes cluster*.
As you step through the quickstart wizard and you are asked to choose a *Trust Model*, select *Kubernetes*.
+
[NOTE]
====
The quickstart wizard will create the necessary YAML files to install the Aporeto components in your Kubernetes cluster through kubectl and Helm commands.
Ensure you have the Helm CLI installed as indicated above and you follow through on the prerequisite steps as indicated in the first section of the quickstart wizard.
====

* xref:../start/enforcer/k8s.adoc[Documentation quickstart]: Requires `apoctl`.
+
[WARNING]
====
Use `cluster1` as the namespace for your first cluster.
If you plan to complete section IV, use `cluster2` as the namespace of your second cluster.
====

[#_deploying-the-hipster-shop-application]
[.task]
=== Deploying the hipster shop application

[.procedure]
. Create the `hipster-dev` namespace in your Kubernetes cluster.
+
[,console]
----
kubectl create namespace hipster-dev
----

. Navigate to the `hipster-dev` namespace in the Aporeto UI that should have automatically been created by the Aporeto operator. We will be returning here once the pods are deployed.
+
image:/img/screenshots/hipster-dev-namespace.png[hipster-dev UI]

. Returning to your terminal prompt, use the following command to deploy the hipster shop in the `hipster-dev` namespace in your Kubernetes cluster.
+
[,console]
----
kubectl create -f \
https://raw.githubusercontent.com/aporeto-inc/microservices-demo/master/release/hipster-dev.yaml
----
+
[TIP]
====
If you are deploying specifically in AWS/EKS, we recommend using `nlb` for the load balancer type.
This recommendation applies throughout the tutorial for any service which is exposed as type load balancer in AWS/EKS.
====

. Check the status of the `hipster-dev` pods and services.
+
[,console]
----
watch kubectl get pods,svc -n hipster-dev
----
+
[NOTE]
====
The above command uses https://linux.die.net/man/1/watch[watch], which is not installed by default on macOS.
While we recommend installing it, you can also omit the `watch` portion of the command and repeatedly issue the command until the pods achieve the necessary status.
====

. This command should return something like the following.
+
----
NAME                                         READY   STATUS    RESTARTS   AGE
pod/adservice-7ffcb6fdd4-846vl               1/1     Running   0          16m
pod/cartservice-64fcb99689-5phc4             1/1     Running   2          21m
pod/checkoutservice-89f9dcf5d-k5jpb          1/1     Running   0          21m
pod/currencyservice-75c9dff8-bfx8c           1/1     Running   0          21m
pod/emailservice-79cf797588-nht76            1/1     Running   0          21m
pod/fake-attacker-758d7c6698-q66nr           1/1     Running   0          21m
pod/frontend-79d9db89d9-z7l4s                1/1     Running   0          21m
pod/loadgenerator-59f7f959dd-lnlbp           1/1     Running   0          21m
pod/paymentservice-6c48cbf74d-tlhsc          1/1     Running   0          21m
pod/productcatalogservice-656d6c65b6-qrhsb   1/1     Running   0          21m
pod/recommendationservice-7c9b6b7796-d9bfl   1/1     Running   0          21m
pod/redis-cart-598c9b7695-m2zsg              1/1     Running   0          21m
pod/shippingservice-85d48cd7bb-b778m         1/1     Running   0          21m
+
NAME                            TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/adservice               ClusterIP      10.35.244.232   +++<none>+++9555/TCP 21m service/cartservice ClusterIP 10.35.249.117 +++<none>+++7070/TCP 21m service/checkoutservice ClusterIP 10.35.248.8 +++<none>+++5050/TCP 21m service/currencyservice ClusterIP 10.35.252.112 +++<none>+++7000/TCP 21m service/emailservice ClusterIP 10.35.250.68 +++<none>+++5000/TCP 21m service/frontend ClusterIP 10.35.241.88 +++<none>+++80/TCP 21m service/frontend-external LoadBalancer 10.35.247.62 34.94.76.6 80:30052/TCP 21m service/paymentservice ClusterIP 10.35.241.63 +++<none>+++50051/TCP 21m service/productcatalogservice ClusterIP 10.35.249.32 +++<none>+++3550/TCP 21m service/recommendationservice ClusterIP 10.35.251.226 +++<none>+++8080/TCP 21m service/redis-cart ClusterIP 10.35.252.161 +++<none>+++6379/TCP 21m service/shippingservice ClusterIP 10.35.255.35 +++<none>+++50051/TCP 21m ```+++</none>++++++</none>++++++</none>++++++</none>++++++</none>++++++</none>++++++</none>++++++</none>++++++</none>++++++</none>++++++</none>+++
----

. Once all of the pods achieve a `STATUS` of `Running`, press CTRL+C to exit `watch`.
+
[NOTE]
====
If you deployed in AWS/EKS you can change the load balancer type for the `frontend-external` service using `kubectl patch`

[,console]
----
kubectl patch svc frontend-external -p \
'{"metadata":{"annotations":{"service.beta.kubernetes.io/aws-load-balancer-type":"nlb"}}}' \
-n hipster-dev
----

====

. Confirm you are able to access the application from an external source such as your laptop browser.
The `frontend-external` service is exposed as a load balancer by default.
If you are using a managed Kubernetes service like EKS, GKE, or AKS the load balancer should be created by automatically.
In the above example the hipster shop should be accessible at the following IP address: `+http://34.94.76.6+`.
. Go shopping in the hipster store and make a fake purchase.
Review the application flows that this generates in in the Aporeto web interface.
. From the Aporeto web interface, navigate to the `hipster-dev` namespace.
. Select *Platform*.
. Copy the following expression to your clipboard.
+
[,console]
----
 $namespace then env then app
----

. Paste it in the grouping expression box to better organize the objects which represent the pods in the corresponding Kubernetes namespace.
+
 image:/img/screenshots/hipster-grouping-expression.png[ui-grouping]
+
[NOTE]
====
You have just entered an ordering of identity value properties present in each of the processing units created which represent the pods in the Kubernetes cluster.
Aporeto allows for free-form grouping based on identity values.
====

. Locate the processing units that were created, each representing the pods in the hipster shop application.
. Select a pod and expand the drop-down to view its identity properties.
. Take notice of the `project=companystore`, the `app=`, and the `env=` identity value in the *User Metadata* section.
We will use these identity values to secure the application in the next sections.
+
[NOTE]
====
* Dotted green lines indicate allowed connections from a default allow policy if no policy is defined.
* Solid green lines indicate successful communication, policy defined.
* Solid red lines indicate blocked communication.
====

. Observe the `fake_attacker` periodically connecting to the pods that are part of the micro-service application.
We will secure the application against this attacker in the next section!

[#_encrypting-pod-to-pod-communications-and-restricting-external-connections]
=== I. Encrypting pod-to-pod communications and restricting external connections

==== Overview

In this section, we show how you can secure a microservices application without in-depth knowledge of its inner workings.
The pod label `project=companystore` automatically becomes a tag in Aporeto.
We use this tag to:

* Allow all of the pods that are a part of the application to communicate with each other.
* Encrypt pod-to-pod communications.
* Restrict pod communications outside of the cluster to the minimum necessary.

==== Importing the external network and network policy definition

We provide a predefined YAML file containing the external networks and network policies.
You can use either of the following methods to import it.

* <<_using-apoctl1,Using `apoctl`>>
* <<_using-the-aporeto-web-interface1,Using the Aporeto web interface>>

[#_using-apoctl1]
===== Using apoctl

If you have xref:../start/apoctl/apoctl.adoc[`apoctl` installed], you can use the following command to import the YAML file.

[,console]
----
cat <<EOF | apoctl api import -n $APOCTL_NAMESPACE/cluster1/hipster-dev -f -
APIVersion: 0
data:
  externalnetworks:
    - associatedTags:
        - 'ext:network=dns'
      description: all dns
      entries:
        - 0.0.0.0/0
      name: dns
      ports:
        - '53'
      protocols:
        - udp
        - tcp
    - associatedTags:
        - 'ext:network=any'
      description: ' any ip'
      entries:
        - 0.0.0.0/0
      name: internet
      protocols:
        - tcp
        - udp
    - associatedTags:
        - 'ext:network=metadata'
      description: cloud metadata
      entries:
        - 169.254.169.254
      name: metadata
      ports:
        - '80'
        - '443'
      protocols:
        - tcp
  networkaccesspolicies:
    - description: allow outbound cloud metadata
      logsEnabled: true
      name: cloud metadata
      object:
        - - 'ext:network=metadata'
      subject:
        - - project=companystore
    - description: ring fence policy
      encryptionEnabled: true
      logsEnabled: true
      name: company store
      object:
        - - project=companystore
      subject:
        - - project=companystore
    - description: allow dns
      name: dns
      object:
        - - 'ext:network=dns'
      subject:
        - - '\$identity=processingunit'
    - description: hipstershop
      logsEnabled: true
      name: frontend-inbound
      object:
        - - app=frontend
      subject:
        - - 'ext:network=any'
    - description: hipstershop
      logsEnabled: true
      name: outbound-allow
      object:
        - - 'ext:network=any'
      subject:
        - - app=emailservice
identities:
  - externalnetwork
  - networkaccesspolicy
label: Free Trial
EOF
----

Skip to <<_reviewing-the-results1,Reviewing the results>>.

[#_using-the-aporeto-web-interface1]
[.task]
===== Using the Aporeto web interface

[.procedure]
. Use the following command to download the `ringfence.yaml` file.
+
[,console]
----
wget \
https://raw.githubusercontent.com/aporeto-inc/microservices-demo/master/release/ringfence.yaml
----

. In the `hipster-dev` namespace in the Aporeto web interface, expand *Data Management* and select *Import/Export*.
. Drag and drop the `ringfence.yaml` file into the *Import* window.
. Select *Import* at the bottom to apply the configuration file.

[#_reviewing-the-results1]
[.task]
==== Reviewing the results

[.procedure]
. In the `hipster-dev` namespace in the Aporeto web interface, expand *Network Authorization* and select *External Networks* to review the external networks you just created.
. Expand *Network Authorization* and select *Policies* to review the policies.
Expand the policy to understand it better.
Click the *Edit* button to understand how network policies can be created from the web interface.
Select *Cancel* to exit.
+
[WARNING]
====
Do not modify any existing policies until you have finished the tutorial.
If you modify any policies, repeat <<_importing-the-external-network-and-network-policy-definition,Importing the external network and network policy definition>>.
====

. Go shopping in the hipster shop and make a fake purchase.
. Select *Platform*.
You may notice some red lines to `Somewhere`.
These lines represent unauthorized `data exfiltration` from your application, blocked by the network policy we just applied.
Notice the connections from the fake attacker (an external source) have begun to turned red, indicating the connections are blocked.
. Click on any green line.
Observe the allowed communication flows under *Access* and associated policy under *Policies*.
Notice the lock icon on the green flows indicating that Aporeto has enabled mutual TLS encryption between the pods in the application.
+
Congratulations!
+
* You have secured the hipster shop.
* You've blocked the attacking pod.
* No IP addresses were used to secure the application.
* The security applied is based on the cryptographically signed identity.

=== II. Restricting pod-to-pod traffic

==== Overview

In this section, we show you how to adopt a stronger security posture, sometimes referred to as zero trust.
We will no longer assume that all pods within the hipster shop application can be trusted.
Instead, we restrict pod-to-pod communications to the minimum necessary.

By blocking unnecessary communications between pods, we can minimize the blast radius of a compromised pod.
For example, if an attacker gains access to the `frontend` pod, they will be unable to reach the `PaymentService` pod.

Each pod has a label defining their role using the following syntax: `app=<role>`.
Our policies use these labels to block unnecessary pod traffic.

At this stage our hipster shop application is still under development.
All of the pods have the label `env-dev`.
In the next section, we will deploy the hipster shop application into production.
The pods in the production hipster shop application will have the label `env=prod`.
We will apply a policy in this section that uses these tags to prevent the pods in the development application from communicating with the pods in the production application.

[#_importing-the-external-network-and-network-policy-definition]
==== Importing the external network and network policy definition

We provide a predefined YAML file containing the external networks and network policies.
You can use either of the following methods to import it.

* <<_using-apoctl2,Using `apoctl`>>
* <<_using-the-poreto-web-interface2,Using the Aporeto web interface>>

[#_using-apoctl2]
===== Using apoctl

If you have xref:../start/apoctl/apoctl.adoc[`apoctl` installed], you can use the following command to import the YAML file.

[,console]
----
cat <<EOF | apoctl api import -n $APOCTL_NAMESPACE/cluster1/hipster-dev -f -
APIVersion: 0
data:
  externalnetworks:
    - associatedTags:
        - 'ext:network=dns'
      description: all dns
      entries:
        - 0.0.0.0/0
      name: dns
      ports:
        - '53'
      protocols:
        - udp
        - tcp
    - associatedTags:
        - 'ext:network=any'
      description: ' any ip'
      entries:
        - 0.0.0.0/0
      name: internet
      protocols:
        - tcp
        - udp
    - associatedTags:
        - 'ext:network=metadata'
      description: cloud metadata
      entries:
        - 169.254.169.254
      name: metadata
      ports:
        - '80'
        - '443'
  networkaccesspolicies:
    - description: allow outbound cloud metadata
      logsEnabled: true
      name: cloud-metadata
      object:
        - - 'ext:network=metadata'
      subject:
        - - project=companystore
    - description: ring fence policy
      disabled: true
      encryptionEnabled: true
      logsEnabled: true
      name: company-store
      object:
        - - project=companystore
      subject:
        - - project=companystore
    - description: allow dns
      name: dns
      object:
        - - 'ext:network=dns'
      subject:
        - - '\$identity=processingunit'
    - description: hipstershop
      logsEnabled: true
      name: frontend-inbound
      object:
        - - app=frontend
      subject:
        - - 'ext:network=any'
    - description: hipstershop
      logsEnabled: true
      name: outbound-allow
      object:
        - - 'ext:network=any'
      subject:
        - - app=emailservice
    - description: hipstershop
      encryptionEnabled: true
      logsEnabled: true
      name: cartservice
      object:
        - - app=redis-cart
      subject:
        - - app=cartservice
    - description: hipstershop
      encryptionEnabled: true
      logsEnabled: true
      name: checkoutservice
      object:
        - - app=emailservice
        - - app=paymentservice
        - - app=shippingservice
        - - app=currencyservice
        - - app=productcatalogservice
        - - app=cartservice
      subject:
        - - app=checkoutservice
    - description: hipstershop
      logsEnabled: true
      name: frontend
      object:
        - - app=adservice
        - - app=checkoutservice
        - - app=shippingservice
        - - app=currencyservice
        - - app=productcatalogservice
        - - app=recommendationservice
        - - app=cartservice
      subject:
        - - app=frontend
    - description: hipstershop
      logsEnabled: true
      name: load-generator
      object:
        - - app=frontend
      subject:
        - - app=loadgenerator
    - description: hipstershop
      encryptionEnabled: true
      logsEnabled: true
      name: recommendationservice
      object:
        - - app=productcatalogservice
      subject:
        - - app=recommendationservice
    - action: Reject
      description: env seperation
      logsEnabled: true
      name: deny-dev-to-prod
      object:
        - - env=prod
      subject:
        - - env=dev
    - action: Reject
      description: env separation
      logsEnabled: true
      name: deny-prod-to-dev
      object:
        - - env=dev
      subject:
        - - env=prod
identities:
  - externalnetwork
  - networkaccesspolicy
label: Free Trial
EOF
----

Skip to <<_reviewing-the-results2,Reviewing the results>>.

[#_using-the-aporeto-web-interface2]
[.task]
===== Using the Aporeto web interface

[.procedure]
. Use the following command to download the `pod-to-pod.yaml` file.
+
[,console]
----
wget \
https://raw.githubusercontent.com/aporeto-inc/microservices-demo/master/release/pod-to-pod.yaml
----

. In the `hipster-dev` namespace in the Aporeto web interface, expand *Data Management* and select *Import/Export*.
. Drag and drop the `pod-to-pod.yaml` file into the *Import* window.
. Select *Import* at the bottom to apply the configuration file.

[#_reviewing-the-results2]
[.task]
==== Reviewing the results

[.procedure]
. In the `hipster-dev` namespace in the Aporeto web interface, expand *Network Authorization* and select *Policies*.
. Review the new network policies, observing:
+
* The network policy we enabled in <<_encrypting-pod-to-pod-communications-and-restricting-external-connections,Encrypting pod-to-pod traffic and restricting external connections>> has been disabled.
* Pods in the development environment cannot communicate with pods in the production environment

. Go shopping in the hipster shop and make a fake purchase on your secured application.
. Select *Platform*.
You will notice some red lines to `Somewhere`.
These lines represent unauthorized `data exfiltration` from your application, blocked by the network policy we just applied.
Notice the connections from the fake attacker have turned red, indicating the connections are blocked.
. Click on any green line.
Observe the allowed communication flows under *Access* and associated policy under *Policies*.
Notice the lock icon on the green flows indicating that Aporeto has enabled mutual TLS encryption between the pods in the application.
+
Congratulations!
+
* You have further secured the Hipster Shop with more granular network policies for a zero trust posture.
* You've blocked the attacking pod.
* No IP addresses were used to secure the application.
* The security applied is based on the cryptographically signed identity.

=== III. Applying network policies as custom resource definitions

==== Overview

Aporeto creates custom resource definitions (CRDs) in Kubernetes.
While you can create, read, update, and delete Aporeto network policy objects through Aporeto, you can alternatively manipulate these objects through the Kubernetes API.
This can provide a smoother integration with your continuous integration and deployment pipelines.

In this section of the tutorial we will export the Aporeto YAML we created in the previous section for our development instance of the hipster shop application, transform it into Kubernetes objects, and use `kubectl` to apply it to a production instance of the same hipster shop application in the same cluster.

==== Prerequisite

This tutorial requires xref:../start/apoctl/apoctl.adoc[`apoctl` to be installed].

[.task]
==== Reviewing the Aporeto custom resource definitions

[.procedure]
. Use the following command to retrieve a list of the Aporeto CRDs.
+
[,console]
----
kubectl get crds | grep aporeto
----

. It should return something like the following.
+
----
externalnetworks.api.aporeto.io                2019-06-30T05:43:28Z
httpresourcespecs.api.aporeto.io               2019-06-30T05:43:28Z
namespacemappingpolicies.api.aporeto.io        2019-06-30T05:43:28Z
namespaces.api.aporeto.io                      2019-06-30T05:43:28Z
networkaccesspolicies.api.aporeto.io           2019-06-30T05:43:28Z
podinjectorselectors.k8s.aporeto.io            2019-06-30T05:43:28Z
servicedependencies.api.aporeto.io             2019-06-30T05:43:28Z
servicemappings.k8s.aporeto.io                 2019-06-30T05:43:28Z
services.api.aporeto.io                        2019-06-30T05:43:28Z
tokenscopepolicies.api.aporeto.io              2019-06-30T05:43:28Z
----

. Use the following command to retrieve an Aporeto network policy CRD.
+
[,console]
----
kubectl describe crds/networkaccesspolicies.api.aporeto.io
----

. It should return something like the following.
+
----
Name:         networkaccesspolicies.api.aporeto.io
Namespace:
Labels:       <none>
Annotations:  <none>
API Version:  apiextensions.k8s.io/v1beta1
Kind:         CustomResourceDefinition
Metadata:
  Creation Timestamp:  2019-06-30T05:43:28Z
  Generation:          1
  Resource Version:    16092
  Self Link:
/apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/networkaccesspolicies.api.aporeto.io
  UID:                 fc6217d2-9af9-11e9-9a35-42010aa80027
Spec:
  Additional Printer Columns:
    JSON Path:    .spec.action
    Description:  List of CIDRs or domain name.
    Name:         action
    Type:         string
 ...
----

[.task]
==== Converting an Aporeto network policy to a Kubernetes CRD

[.procedure]
. Create the `hipster-prod` namespace in Kubernetes.
+
[,console]
----
kubectl create namespace hipster-prod
----

. Convert the `pod-to-pod.yaml` developed in `hipster-dev` into a Kubernetes CRD and apply it a production instance which will run in the `hipster-prod` namespace.
+
[,console]
----
apoctl api import \
--url https://raw.githubusercontent.com/aporeto-inc/microservices-demo/master/release/pod-to-pod.yaml \
--to-k8s-crd \
| sed -e 's/aporeto.io\/v1alpha1/api.aporeto.io\/v1beta1/g' \
| kubectl create -f - -n hipster-prod
----

. Use the following commands to explore the network policies and external networks you just created.
+
[,console]
----
kubectl get networkaccesspolicies.api.aporeto.io \
  -n hipster-prod
----
+
[,console]
----
kubectl describe networkaccesspolicies.api.aporeto.io/frontend \
  -n hipster-prod
----
+
[,console]
----
kubectl get externalnetworks.api.aporeto.io \
  -n hipster-prod
----
+
[,console]
----
kubectl describe externalnetworks.api.aporeto.io/dns \
  -n hipster-prod
----

[.task]
==== Deploying the hipster shop application

[.procedure]
. Use the following command to deploy the hipster shop application into the `hipster-prod` namespace.
+
[,console]
----
kubectl create -f \
https://raw.githubusercontent.com/aporeto-inc/microservices-demo/master/release/hipster-prod.yaml
----

. Check on the status of the hipster shop deployment.
+
[,console]
----
kubectl get pods,svc -n hipster-prod
----

. Ensure that all pods and services achieve `Running` status and copy the `External-IP` of the `frontend-external` service.
. Paste the `External-IP` of the `frontend-external` service into a browser and confirm that you can place an order.

[.task]
==== Reviewing the results

[.procedure]
. Navigate to the `hipster-prod` namespace in the Aporeto web interface.
. Expand *Network Authorization* and select *External Networks* to review the external networks you just created.
. Expand *Network Authorization* and select *Policies* to review the policies.
. Select *Platform*.
You will notice some red lines to `Somewhere`.
These lines represent unauthorized `data exfiltration` from your application, blocked by the network policy we just applied.
Notice the connections from the fake attacker have turned red, indicating the connections are blocked.
. Click on any green line.
Observe the allowed communication flows under *Access* and associated policy under *Policies*.
Notice the lock icon on the green flows indicating that Aporeto has enabled mutual TLS encryption between the pods in the application.
+
Congratulations!
+
* You have secured your production application using the policies created in your development environment.
* Notice the attacking pod is also blocked.
* The identity-based policy model has been carried over and into the Kubernetes cluster using CRDs.

=== IV. Securing cross-cluster applications

==== Overview

In this section, we secure a production instance of hipster shop application that's split across two Kubernetes clusters.
The following diagram shows its split architecture.

[cols="1a,1a"]
|===
|`cluster1` |`cluster2`

|image:/img/diagrams/hipster-svc-arch1.png[svc-arch-c1]
|image:/img/diagrams/hipster-svc-arch2.png[svc-arch-c2]
|===

We will export the Aporeto YAML we created in the previous sections for our development instance of the hipster shop application, and import the YAML into a different namespace in the Aporeto platform to secure the split cluster production instance of the same hipster shop application.

==== Prerequisites

This section requires a <<_setting-up-your-cluster,second Kubernetes cluster named `cluster2` with Aporeto installed>>.

[.task]
==== Preparing the clusters

[.procedure]
. To save resources, go ahead and delete the `hipster-prod` namespace.
We won't be needing this any longer.
+
[,console]
----
kubectl delete namespace hipster-prod
----

. Create a `hipster-multi` namespace in both clusters. Assuming you have multiple clusters defined in your `kube config`, you can use the below commands.
+
[,console]
----
kubectl config get-contexts

kubectl config use-context {{$CLUSTER1}}
kubectl create namespace hipster-multi

kubectl config use-context {{$CLUSTER2}}
kubectl create namespace hipster-multi
----

[.task]
==== Importing the external networks and network policies

In this section, we'll export the policies from the `hipster-dev` namespace and import them into the `hipster-prod` namespace on each of the clusters.

[TIP]
====
If you did not complete the previous sections, run the commands below and skip to <<_deploying-the-split-application,Deploying the split application>>.

[,console]
----
apoctl api import \
  -n $APOCTL_NAMESPACE/cluster1/hipster-multi \
  --url https://raw.githubusercontent.com/aporeto-inc/microservices-demo/master/release/pod-to-pod.yaml
----

[,console]
----
apoctl api import \
  -n $APOCTL_NAMESPACE/cluster2/hipster-multi \
  --url https://raw.githubusercontent.com/aporeto-inc/microservices-demo/master/release/pod-to-pod.yaml
----

====

[.procedure]
. Export the external network definitions from the `cluster1/hipster-dev` namespace.
+
[,console]
----
apoctl api -n $APOCTL_NAMESPACE/cluster1/hipster-dev \
export externalnetworks > hipster_ext_net.yaml
----

. Export the network policy definitions from the `cluster1/hipster-dev` namespace.
+
[,console]
----
apoctl api -n $APOCTL_NAMESPACE/cluster1/hipster-dev \
export networkaccesspolicy > hipster_netpol.yaml
----

. Import the exported external network definition into the `cluster1/hipster-multi` namespace and the `cluster2/hipster-multi` namespace.
+
[,console]
----
apoctl api -n $APOCTL_NAMESPACE/cluster1/hipster-multi \
import -f hipster_ext_net.yaml
apoctl api -n $APOCTL_NAMESPACE/cluster2/hipster-multi \
import -f hipster_ext_net.yaml
----

. Import the exported network policy into the `cluster1/hipster-multi` namespace and the `cluster2/hipster-multi` namespace.
+
[,console]
----
apoctl api -n $APOCTL_NAMESPACE/cluster1/hipster-multi \
import -f hipster_netpol.yaml
apoctl api -n $APOCTL_NAMESPACE/cluster2/hipster-multi \
import -f hipster_netpol.yaml
----

[#_deploying-the-split-application]
[.task]
==== Deploying the split application

[.procedure]
. With your `kubectl` context set to `cluster2` issue the following command to apply our `svc-cluster-2.yaml` file.
+
[,console]
----
kubectl create -f \
https://raw.githubusercontent.com/aporeto-inc/microservices-demo/master/release/svc-cluster-2.yaml
----

. It should return the following.
+
----
service/cartservice created
service/frontend created
service/frontend-external created
service/productcatalogservice created
service/recommendationservice created
service/redis-cart created
----

. Use the following command to review the services you just deployed in `cluster2`.
+
[,console]
----
kubectl get svc -n hipster-multi
----

. It should return something like the following.
+
----
NAME                    TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)            AGE
cartservice             LoadBalancer   10.15.253.38    104.154.186.47   7070:30251/TCP     2m37s
frontend                ClusterIP      10.15.250.183   <none>           80/TCP             2m37s
frontend-external       LoadBalancer   10.15.249.187   34.68.150.252    80:30920/TCP       2m37s
productcatalogservice   LoadBalancer   10.15.250.157   35.184.250.205   3550:32420/TCP     2m36s
recommendationservice   ClusterIP      10.15.240.154   <none>           8080/TCP           2m36s
redis-cart              ClusterIP      10.15.255.215   <none>           6379/TCP           2m36s
----
+
[WARNING]
====
Ensure that the `EXTERNAL-IP` of the `LoadBalancer` services have populated before proceeding.
It may take a few minutes.
If these fields are not expected to be populated you can manually set the required environment variables.
====

. Run the command below to automatically set environment variables which will be used in the later steps.
+
[,console]
----
export $(kubectl get svc -n hipster-multi \
       -o jsonpath='{range.items[?(@.spec.type=="LoadBalancer")]}{.metadata.name}_SVC={.status.loadBalancer.ingress[].hostname}{.status.loadBalancer.ingress[].ip}{"\n"}' \
       | sed 's/frontend-/frontend/g' \
       | awk -F[=] '{ print toupper($1)"="$2 }'
----

. Alternatively, you can manually set the required environment variables.
The values shown below are just examples.
These examples match the example response from `kubectl get svc` above.
+
----
export CARTSERVICE_SVC=104.154.186.47
export FRONTENDEXTERNAL_SVC=34.68.150.252
export PRODUCTCATALOGSERVICE_SVC=35.184.250.205
----

. Switch your kubeconfig context to `cluster1`, as shown below.
+
[,console]
----
kubectl config use-context {{$CLUSTER1}}
----

. Issue the following command to apply our `svc-cluster-1.yaml` file to `cluster1`.
+
[,console]
----
kubectl create -f \
https://raw.githubusercontent.com/aporeto-inc/microservices-demo/master/release/svc-cluster-1.yaml
----

. It should return something like the following.
+
----
service/emailservice created
service/checkoutservice created
service/paymentservice created
service/currencyservice created
service/shippingservice created
service/adservice created
----

. Use the following command to review the services you just deployed in `cluster1`.
+
[,console]
----
kubectl get svc -n hipster-multi
----

. It should return something like the following.
+
----
NAME              TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)           AGE
adservice         LoadBalancer   10.35.251.219   34.94.90.88    9555:31847/TCP    3m45s
checkoutservice   LoadBalancer   10.35.250.45    34.94.68.198   5050:31845/TCP    3m46s
currencyservice   LoadBalancer   10.35.250.72    34.94.108.84   7000:32157/TCP    3m46s
emailservice      ClusterIP      10.35.245.81    <none>         5000/TCP          3m47s
paymentservice    ClusterIP      10.35.243.196   <none>         50051/TCP         3m46s
shippingservice   LoadBalancer   10.35.241.119   34.94.50.52    50051:30924/TCP   3m46s
----
+
[WARNING]
====
Ensure that the `EXTERNAL-IP` of the `LoadBalancer` services have populated before proceeding.
It may take a few minutes.
If these fields are not expected to be populated you can manually set the required environment variables.
====

. Run the command below to automatically set environment variables which will be used in the later steps.
+
[,console]
----
export $(kubectl get svc -n hipster-multi \
       -o jsonpath='{range.items[?(@.spec.type=="LoadBalancer")]}{.metadata.name}_SVC={.status.loadBalancer.ingress[].hostname}{.status.loadBalancer.ingress[].ip}{"\n"}' \
       | sed 's/frontend-/frontend/g' \
       | awk -F[=] '{ print toupper($1)"="$2 }'
----

. Alternatively, you can manually set the required environment variables.
The values shown below are just examples.
These examples match the example response from `kubectl get svc` above.
+
----
export ADSERVICE_SVC=34.94.90.88
export CHECKOUTSERVICE_SVC=34.94.68.198
export CURRENCYSERVICE_SVC=34.94.108.84
export SHIPPINGSERVICE_SVC=34.94.50.52
----

. Confirm the necessary environment variables have been set.
Manually set them if they are not.
+
[,console]
----
env | grep _SVC=
----

. It should return something like the following.
+
----
CARTSERVICE_SVC=104.154.186.47
FRONTENDEXTERNAL_SVC=34.68.150.252
PRODUCTCATALOGSERVICE_SVC=35.184.250.205
ADSERVICE_SVC=34.94.90.88
CHECKOUTSERVICE_SVC=34.94.68.198
CURRENCYSERVICE_SVC=34.94.108.84
SHIPPINGSERVICE_SVC=34.94.50.52
----

. Apply the step 3 variables to the `cluster1` deployment file using the command below:
+
[,console]
----
wget -O- -q https://raw.githubusercontent.com/aporeto-inc/microservices-demo/master/release/deployment-cluster-1.yaml \
| sed \
-e "s/{{PRODUCTCATALOG_SERVICE}}/$PRODUCTCATALOGSERVICE_SVC/g" \
-e "s/{{CART_SERVICE}}/$CARTSERVICE_SVC/g" \
| kubectl create -f - -n hipster-multi
----

. Switch contexts to `cluster2`.
+
[,console]
----
kubectl config use-context {{$CLUSTER2}}
----

. Apply the step 7 variables to the `cluster2` deployment file using the command below:
+
[,console]
----
wget -O- -q https://raw.githubusercontent.com/aporeto-inc/microservices-demo/master/release/deployment-cluster-2.yaml \
| sed \
-e "s/{{CURRENCY_SERVICE}}/$CURRENCYSERVICE_SVC/g" \
-e "s/{{SHIPPING_SERVICE}}/$SHIPPINGSERVICE_SVC/g" \
-e "s/{{CHECKOUT_SERVICE}}/$CHECKOUTSERVICE_SVC/g" \
-e "s/{{AD_SERVICE}}/$ADSERVICE_SVC/g" \
| kubectl create -f - -n hipster-multi
----

. At this point the application should be fully deployed and accessible!
Access `frontend-external` from a browser and ensure you can browse the hipster shop.
Recall the IP address was saved as an environment variable.
+
[,console]
----
env | grep FRONTEND
----

[.task]
==== Go shopping in the hipster shop

[.procedure]
. Provided you have available resources in your cluster, scale out the product catalog deployment on `cluster2`.
+
[,console]
----
kubectl scale --replicas=3 deployment/productcatalogservice \
  -n hipster-multi
----

. In the Aporeto web interface, navigate to the parent namespace of `cluster1` and `cluster2` and select *Platform*.
. Copy and paste the following string into the *Enter a filter* box.
+
[,console]
----
namespace matches hipster-multi
----
+
You should see a view of the split application running across two clusters.
+
[TIP]
====
If you don't see flows either access the application again or change the reported flows to the last five minutes.
Move the groups around.
Use two fingers to zoom in and out to create a comfortable view.
====
+
Congratulations!
+
* You have secured your production, multi-cluster application instance with the same identity-based policy used to secure the single cluster development instance.
* The communication between the pods, cross-cluster is encrypted and secured.
* The attacking pod is also blocked.
* The network policy did not have to be updated as you scaled the `productcatalog` micro-service.
